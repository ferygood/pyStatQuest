[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pyStatQuest",
    "section": "",
    "text": "這是第一頁"
  },
  {
    "objectID": "posts/pca/pca.html",
    "href": "posts/pca/pca.html",
    "title": "1  PCA (Singular Value Decomposition)",
    "section": "",
    "text": "1.0.2 實際操作\n我們從影片可以知道 PCA 主要是用來分群資料做 clustering，他的評斷方式主要為計算 Sum of square of distances / n-1 得到變異數 (variance)後計算每個 PC 佔多少 variance 的比例，通常可以用 scree plot 來呈現。所以以下實際示範我們從一個預設或是隨機生成的資料集之中進行 PCA 分析，並繪製 PC1, PC2 的圖以及 scree plot. PCA 為一種 Unsupervised dimensionality reduction 的技術。\n\ncocktail recipe, linear combination\nsingular vector, Eigenvector, loading score\n\n\n\n1.0.3 補充概念\nPCA 是一種類型的 Singular Value Decomposition (SVD)，其目的在於解釋變異數 Variances。\nPCA 概念為將數據透過線性投影的方式進行降維 linear dimensionality reduction ，而將數據降維的好處：\n\nDimension 可看成 feature\n數據在低維度較好理解跟做分析\n可分析相關特徵，特別是可以到二維或是三維的情況，對於視覺化很有幫助\n移除數據裡面的雜訊\n減少數據計算成本\n\n\n\n1.0.4 PCA 其他應用\n\nFactor Analysis\nIndependent Component Analysis (ICA)\n\n\n\n1.0.5 程式碼範例 1\n\n1.0.5.1 使用 iris data\n我們使用 plotly 內建的 iris 資料，選取四個 features 將其用散佈圖視覺化：\n\nimport plotly.express as px\nimport pandas as pd\n\n# load iris data\ndf = px.data.iris()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n      species_id\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n      1\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n      1\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n      1\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n      1\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n      1\n    \n  \n\n\n\n\n\n# create scatter plot using plotly\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n\nfig = px.scatter_matrix(\n    df,\n    dimensions=features,\n    color=\"species\"\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n                                                \n\n\n\n\n1.0.5.2 sklearn PCA\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\ncomponents = pca.fit_transform(df[features])\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(4),\n    color=df[\"species\"]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n                                                \n\n\n從 PCA 結果可以看出，xy 軸用 PC1 (92.5%), PC2 (5.3%) 可以有效分開不同的品種。\n\n\n\n1.0.6 程式碼範例 2\n由於範例 1 為相同 scale 且只有四個 features 較單純，而 PCA 線性轉換對 scale 敏感，所以範例 2 我們使用標準化的方法。使用 seaborn 套件內建的資料 diamonds 來做 PCA 分析。\n\n# load packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn import decomposition\n\ndiamond = sns.load_dataset(\"diamonds\")\ndiamond.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\n從資料可以看出 features 有 carat, cut, color, clarity, depth, table, price, x, y, z。\n我們先設定來看 PCA 能不能區分顏色，所以第一步驟是將所有非數字的欄位轉成 dummy variables。\n\n# get dummies and store it in a variable\ndummies = pd.get_dummies(diamond[[\"cut\", \"clarity\"]])\n\n# concat dummies to original dataframe and drop values\nmerged = pd.concat([diamond, dummies], axis='columns')\nmerged.drop(['cut', 'clarity'], axis='columns', inplace=True)\n\n#random select rows\nmerged = merged.sample(n=500)\n\nprint(merged.describe())\n\n            carat       depth       table         price           x  \\\ncount  500.000000  500.000000  500.000000    500.000000  500.000000   \nmean     0.776060   61.653600   57.539600   3852.526000    5.680800   \nstd      0.467365    1.578706    2.308352   4025.766604    1.116457   \nmin      0.200000   50.800000   52.000000    367.000000    3.760000   \n25%      0.390000   61.000000   56.000000    921.000000    4.687500   \n50%      0.645000   61.800000   57.000000   2160.500000    5.570000   \n75%      1.022500   62.400000   59.000000   5280.000000    6.510000   \nmax      2.480000   69.400000   76.000000  18705.000000    8.640000   \n\n                y           z   cut_Ideal  cut_Premium  cut_Very Good  \\\ncount  500.000000  500.000000  500.000000   500.000000     500.000000   \nmean     5.682400    3.501660    0.420000     0.256000       0.208000   \nstd      1.109727    0.685557    0.494053     0.436859       0.406283   \nmin      3.730000    2.330000    0.000000     0.000000       0.000000   \n25%      4.687500    2.897500    0.000000     0.000000       0.000000   \n50%      5.555000    3.415000    0.000000     0.000000       0.000000   \n75%      6.472500    4.010000    1.000000     1.000000       0.000000   \nmax      8.550000    5.450000    1.000000     1.000000       1.000000   \n\n         cut_Good    cut_Fair  clarity_IF  clarity_VVS1  clarity_VVS2  \\\ncount  500.000000  500.000000   500.00000    500.000000    500.000000   \nmean     0.076000    0.040000     0.04800      0.060000      0.108000   \nstd      0.265264    0.196155     0.21398      0.237725      0.310691   \nmin      0.000000    0.000000     0.00000      0.000000      0.000000   \n25%      0.000000    0.000000     0.00000      0.000000      0.000000   \n50%      0.000000    0.000000     0.00000      0.000000      0.000000   \n75%      0.000000    0.000000     0.00000      0.000000      0.000000   \nmax      1.000000    1.000000     1.00000      1.000000      1.000000   \n\n       clarity_VS1  clarity_VS2  clarity_SI1  clarity_SI2  clarity_I1  \ncount   500.000000   500.000000   500.000000     500.0000  500.000000  \nmean      0.168000     0.216000     0.250000       0.1420    0.008000  \nstd       0.374241     0.411926     0.433446       0.3494    0.089173  \nmin       0.000000     0.000000     0.000000       0.0000    0.000000  \n25%       0.000000     0.000000     0.000000       0.0000    0.000000  \n50%       0.000000     0.000000     0.000000       0.0000    0.000000  \n75%       0.000000     0.000000     0.250000       0.0000    0.000000  \nmax       1.000000     1.000000     1.000000       1.0000    1.000000  \n\n\n現在除了 color 之外，其他的都轉成數字了，但因為數字的 min, max, std 這些數值的級距不同，所以在做 PCA 之前先進行一個標準化的動作。\n\npca = decomposition.PCA()\npc = pca.fit_transform(merged.loc[:, merged.columns!='color'])\npc_df = pd.DataFrame(data=pc)\npc_df.head()\n\ndf = pd.DataFrame({\n  'var': pca.explained_variance_ratio_,\n  'PC':[\"PC\" + str(i) for i in list(range(1,21))]\n  })\n\nsns.barplot(x = 'PC', y='var', data=df, color=\"c\")\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n上圖為未經標準化的 variance。\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nmerged_scale = scaler.fit_transform(merged.loc[:, merged.columns!='color'])\n\npca = decomposition.PCA()\npc_scale = pca.fit_transform(merged_scale)\npc_df_scale = pd.DataFrame(pc_scale, columns = [\"PC\" + str(i) for i in list(range(1,21))])\npc_df_scale['color'] = merged.color\n\ndf_scale = pd.DataFrame({\n  'var': pca.explained_variance_ratio_,\n  'PC': [\"PC\" + str(i) for i in list(range(1,21))]\n})\n\nsns.barplot(x = 'PC', y='var', data=df_scale, color=\"b\")\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n上圖為經過 standard scaler 標準化的圖\n\nsns.lmplot(\n  x=\"PC1\",\n  y=\"PC2\",\n  data=pc_df_scale,\n  hue=\"color\",\n  fit_reg = False,\n  legend=True,\n  scatter_kws={\"s\": 40}\n)\n\n 結果不是很理想，看起來這個資料集用 PCA 不是很好做顏色的 clustering。但我們還是呈現結果，並繪製 scree plot。之後有時間可以在嘗試不同的方法，但這個第二個部分主要是呈現有沒有標準化對於 variance 的影響。\nscree plot using sns.scatterplot\n\npc_value = np.arange(pca.n_components_) + 1\nsns.scatterplot(\n  x='PC',\n  y='var',\n  data=df_scale\n)\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n\n\n1.0.7 重點回顧\n\nPCA 是一個線性降維的技術\n步驟是先將資料都轉成數值型，並標準化（normalization, standardization) 處理完之後進行 PCA 分析\n評斷結果好壞可以從視覺化下手：(1) xy 軸為 PC1, PC2 的分佈 (2) Scree plot\n理想上會希望前幾個 PCs 就能夠對於 Variance 有足夠的代表性 (Ex: 超過 70%)。\n\n\n\n1.0.8 參考資料\nPCA 算法目的作用以及解析\nPlotly PCA 的教學\n標準化 PCA 教學\nPython and R tips"
  },
  {
    "objectID": "posts/logistic_regression/logistic_regression.html",
    "href": "posts/logistic_regression/logistic_regression.html",
    "title": "2  Logistic Regression",
    "section": "",
    "text": "這是 StatQuest 講解 Logistic Regression 的筆記：\n\n\n重點節錄：\n\n可用於統計與機器學習\n用在分類的問題，判斷預測是 True/ False\n判斷變數對於預測有沒有幫助 (透過 Walid Test)\nfit the line with Maximum-likelihood\n\n看完之後我們利用 python 來做一個重點的回顧與練習。"
  },
  {
    "objectID": "posts/likelihood/likelihood.html",
    "href": "posts/likelihood/likelihood.html",
    "title": "3  Maximum likelihood",
    "section": "",
    "text": "這是 StatQuest 講解 likelihood 以及 maximum likelihood 的筆記：\nProbability is not likelihood! 影片以常態分佈為例： \nMaximum likelihood, clearly explained! \n重點節錄："
  },
  {
    "objectID": "posts/p-value/p-value.html",
    "href": "posts/p-value/p-value.html",
    "title": "4  p-value",
    "section": "",
    "text": "這是 StatQuest 講解 p-value 的筆記：\n\n\n這部影片用丟硬幣來當作例子。 以下為影片提到 p-value 的定義：\n\n\n\n\n\n\nNote\n\n\n\nA p-value is the probabillity that random chance generated the data, or something else that is equal or rarer.\n\n\n我嘗試以中文解釋為，p-value 代表你取樣該 data 之機率在整體分佈的稀有度。Part1 為計算你取樣的 data 在機率分布 (density) 的大小，Part2 為加總所有跟你取樣一樣機率大小以及更低的事件總和，這個總和為 p-value\n 以這張截圖的例子來說，你丟硬幣得到 HH 的 p-value 為何？首先你先計算 HH 和 TT 的機率各為 0.25，然而其他的可能性機率都比較高，所以你的 part1: 0.25 + 0.25 = 0.5，你的 part2: 0。所以 HH 的 p-value 為 0.5。"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]