[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pyStatQuest",
    "section": "",
    "text": "這是第一頁"
  },
  {
    "objectID": "posts/pca/pca.html",
    "href": "posts/pca/pca.html",
    "title": "1  PCA (Singular Value Decomposition)",
    "section": "",
    "text": "1.0.2 實際操作\n我們從影片可以知道 PCA 主要是用來分群資料做 clustering，他的評斷方式主要為計算 Sum of square of distances / n-1 得到變異數 (variance)後計算每個 PC 佔多少 variance 的比例，通常可以用 scree plot 來呈現。所以以下實際示範我們從一個預設或是隨機生成的資料集之中進行 PCA 分析，並繪製 PC1, PC2 的圖以及 scree plot. PCA 為一種 Unsupervised dimensionality reduction 的技術。\n\ncocktail recipe, linear combination\nsingular vector, Eigenvector, loading score\n\n\n\n1.0.3 補充概念\nPCA 是一種類型的 Singular Value Decomposition (SVD)，其目的在於解釋變異數 Variances。\nPCA 概念為將數據透過線性投影的方式進行降維 linear dimensionality reduction ，而將數據降維的好處：\n\nDimension 可看成 feature\n數據在低維度較好理解跟做分析\n可分析相關特徵，特別是可以到二維或是三維的情況，對於視覺化很有幫助\n移除數據裡面的雜訊\n減少數據計算成本\n\n\n\n1.0.4 PCA 其他應用\n\nFactor Analysis\nIndependent Component Analysis (ICA)\n\n\n\n1.0.5 程式碼範例 1\n\n1.0.5.1 使用 iris data\n我們使用 plotly 內建的 iris 資料，選取四個 features 將其用散佈圖視覺化：\n\nimport plotly.express as px\nimport pandas as pd\n\n# load iris data\ndf = px.data.iris()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n      species_id\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n      1\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n      1\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n      1\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n      1\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n      1\n    \n  \n\n\n\n\n\n# create scatter plot using plotly\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n\nfig = px.scatter_matrix(\n    df,\n    dimensions=features,\n    color=\"species\"\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n                                                \n\n\n\n\n1.0.5.2 sklearn PCA\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\ncomponents = pca.fit_transform(df[features])\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(4),\n    color=df[\"species\"]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n                                                \n\n\n從 PCA 結果可以看出，xy 軸用 PC1 (92.5%), PC2 (5.3%) 可以有效分開不同的品種。\n\n\n\n1.0.6 程式碼範例 2\n由於範例 1 為相同 scale 且只有四個 features 較單純，而 PCA 線性轉換對 scale 敏感，所以範例 2 我們使用標準化的方法。使用 seaborn 套件內建的資料 diamonds 來做 PCA 分析。\n\n# load packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn import decomposition\n\ndiamond = sns.load_dataset(\"diamonds\")\ndiamond.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\n從資料可以看出 features 有 carat, cut, color, clarity, depth, table, price, x, y, z。\n我們先設定來看 PCA 能不能區分顏色，所以第一步驟是將所有非數字的欄位轉成 dummy variables。\n\n# get dummies and store it in a variable\ndummies = pd.get_dummies(diamond[[\"cut\", \"clarity\"]])\n\n# concat dummies to original dataframe and drop values\nmerged = pd.concat([diamond, dummies], axis='columns')\nmerged.drop(['cut', 'clarity'], axis='columns', inplace=True)\n\n#random select rows\nmerged = merged.sample(n=500)\n\nprint(merged.describe())\n\n           carat      depth       table         price           x           y  \\\ncount  500.00000  500.00000  500.000000    500.000000  500.000000  500.000000   \nmean     0.82142   61.75620   57.533400   4173.454000    5.792360    5.796280   \nstd      0.48153    1.34975    2.275042   4152.459643    1.116733    1.109315   \nmin      0.23000   56.20000   53.000000    351.000000    3.880000    3.920000   \n25%      0.41000   61.10000   56.000000    988.500000    4.770000    4.780000   \n50%      0.71000   61.90000   57.000000   2602.500000    5.720000    5.740000   \n75%      1.07000   62.60000   59.000000   5889.250000    6.632500    6.622500   \nmax      3.67000   65.80000   70.000000  18818.000000    9.860000    9.810000   \n\n                z   cut_Ideal  cut_Premium  cut_Very Good    cut_Good  \\\ncount  500.000000  500.000000   500.000000     500.000000  500.000000   \nmean     3.578520    0.394000     0.270000       0.210000    0.106000   \nstd      0.690612    0.489124     0.444404       0.407716    0.308146   \nmin      2.390000    0.000000     0.000000       0.000000    0.000000   \n25%      2.970000    0.000000     0.000000       0.000000    0.000000   \n50%      3.555000    0.000000     0.000000       0.000000    0.000000   \n75%      4.060000    1.000000     1.000000       0.000000    0.000000   \nmax      6.130000    1.000000     1.000000       1.000000    1.000000   \n\n        cut_Fair  clarity_IF  clarity_VVS1  clarity_VVS2  clarity_VS1  \\\ncount  500.00000  500.000000    500.000000    500.000000   500.000000   \nmean     0.02000    0.026000      0.040000      0.110000     0.180000   \nstd      0.14014    0.159295      0.196155      0.313203     0.384572   \nmin      0.00000    0.000000      0.000000      0.000000     0.000000   \n25%      0.00000    0.000000      0.000000      0.000000     0.000000   \n50%      0.00000    0.000000      0.000000      0.000000     0.000000   \n75%      0.00000    0.000000      0.000000      0.000000     0.000000   \nmax      1.00000    1.000000      1.000000      1.000000     1.000000   \n\n       clarity_VS2  clarity_SI1  clarity_SI2  clarity_I1  \ncount   500.000000   500.000000   500.000000  500.000000  \nmean      0.230000     0.218000     0.186000    0.010000  \nstd       0.421254     0.413301     0.389496    0.099598  \nmin       0.000000     0.000000     0.000000    0.000000  \n25%       0.000000     0.000000     0.000000    0.000000  \n50%       0.000000     0.000000     0.000000    0.000000  \n75%       0.000000     0.000000     0.000000    0.000000  \nmax       1.000000     1.000000     1.000000    1.000000  \n\n\n現在除了 color 之外，其他的都轉成數字了，但因為數字的 min, max, std 這些數值的級距不同，所以在做 PCA 之前先進行一個標準化的動作。\n\npca = decomposition.PCA()\npc = pca.fit_transform(merged.loc[:, merged.columns!='color'])\npc_df = pd.DataFrame(data=pc)\npc_df.head()\n\ndf = pd.DataFrame({\n  'var': pca.explained_variance_ratio_,\n  'PC':[\"PC\" + str(i) for i in list(range(1,21))]\n  })\n\nsns.barplot(x = 'PC', y='var', data=df, color=\"c\")\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n上圖為未經標準化的 variance。\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nmerged_scale = scaler.fit_transform(merged.loc[:, merged.columns!='color'])\n\npca = decomposition.PCA()\npc_scale = pca.fit_transform(merged_scale)\npc_df_scale = pd.DataFrame(pc_scale, columns = [\"PC\" + str(i) for i in list(range(1,21))])\npc_df_scale['color'] = merged.color\n\ndf_scale = pd.DataFrame({\n  'var': pca.explained_variance_ratio_,\n  'PC': [\"PC\" + str(i) for i in list(range(1,21))]\n})\n\nsns.barplot(x = 'PC', y='var', data=df_scale, color=\"b\")\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n上圖為經過 standard scaler 標準化的圖\n\nsns.lmplot(\n  x=\"PC1\",\n  y=\"PC2\",\n  data=pc_df_scale,\n  hue=\"color\",\n  fit_reg = False,\n  legend=True,\n  scatter_kws={\"s\": 40}\n)\n\n 結果不是很理想，看起來這個資料集用 PCA 不是很好做顏色的 clustering。但我們還是呈現結果，並繪製 scree plot。之後有時間可以在嘗試不同的方法，但這個第二個部分主要是呈現有沒有標準化對於 variance 的影響。\nscree plot using sns.scatterplot\n\npc_value = np.arange(pca.n_components_) + 1\nsns.scatterplot(\n  x='PC',\n  y='var',\n  data=df_scale\n)\n\n<AxesSubplot:xlabel='PC', ylabel='var'>\n\n\n\n\n\n\n\n1.0.7 重點回顧\n\nPCA 是一個線性降維的技術\n步驟是先將資料都轉成數值型，並標準化（normalization, standardization) 處理完之後進行 PCA 分析\n評斷結果好壞可以從視覺化下手：(1) xy 軸為 PC1, PC2 的分佈 (2) Scree plot\n理想上會希望前幾個 PCs 就能夠對於 Variance 有足夠的代表性 (Ex: 超過 70%)。\n\n\n\n1.0.8 參考資料\nPCA 算法目的作用以及解析\nPlotly PCA 的教學\n標準化 PCA 教學\nPython and R tips"
  },
  {
    "objectID": "posts/logistic_regression/logistic_regression.html",
    "href": "posts/logistic_regression/logistic_regression.html",
    "title": "2  Logistic Regression",
    "section": "",
    "text": "這是 StatQuest 講解 Logistic Regression 的筆記：\n\n\n重點節錄：\n\n可用於統計與機器學習\n用在分類的問題，判斷預測是 True/ False\n判斷變數對於預測有沒有幫助 (透過 Walid Test)\nfit the line with Maximum-likelihood\n\n看完之後我們利用 python 來做一個重點的回顧與練習。"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]